{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##PRAGYA RAGHUVANSHI\n",
        "Net ID: pr158"
      ],
      "metadata": {
        "id": "O6XV8EJxLO7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Installing dependencies and packages"
      ],
      "metadata": {
        "id": "_jGUw7zmWy6E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kYp4IWrALJbE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install pyglet==1.4\n",
        "!pip install tensorflow==2.0.0-alpha0\n",
        "!pip install autorom\n",
        "!AutoROM --accept-license\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://github.com/DLR-RM/stable-baselines3 && cd stable-baselines3\n",
        "!pip install -e .[docs,tests,extra]"
      ],
      "metadata": {
        "id": "bFrrO5LpT0DS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Library Imports\n",
        "import gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "logdir = ('./cartpole_tb_3')\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwSeucgoMZ79",
        "outputId": "42539317-c67b-4c8c-ca74-6981478b65bc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GYM Environments For Training & Evaluation"
      ],
      "metadata": {
        "id": "8-zARBANMdlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#HYPERPARAMETER\n",
        "training_steps = 10000"
      ],
      "metadata": {
        "id": "jeep_mAdNL2E"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training environment of Cartpole\n",
        "train_env = gym.make('CartPole-v1')\n",
        "\n",
        "## Evalutation environment of Cartpole\n",
        "evaluation_env = gym.make('CartPole-v1')"
      ],
      "metadata": {
        "id": "DeDpfsLoMeZk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vanilla model\n",
        "van_model = A2C('MlpPolicy', train_env, verbose = 0, tensorboard_log=logdir , gae_lambda = 0) \n",
        "\n",
        "# GAE advantage model\n",
        "gae_model = A2C('MlpPolicy', train_env, verbose = 0, tensorboard_log=logdir , gae_lambda = 0.85)\n",
        "\n",
        "#n-step advantage model\n",
        "nstep_model = A2C('MlpPolicy', train_env, verbose = 0, tensorboard_log=logdir , gae_lambda = 1, n_steps = 5)"
      ],
      "metadata": {
        "id": "tRUQDS_JMiZ2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Rewards of the model \n",
        "1. Vanilla Model"
      ],
      "metadata": {
        "id": "caFbACWPOyEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training \n",
        "van_model.learn(total_timesteps=training_steps)\n",
        "#evaluating performance\n",
        "vanillareward_mean, vanillareward_std= evaluate_policy(van_model, evaluation_env, n_eval_episodes=100)\n",
        "\n"
      ],
      "metadata": {
        "id": "V5AFS1PsNg8y"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. GAE model "
      ],
      "metadata": {
        "id": "l6lfU-u2Pkdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training \n",
        "gae_model.learn(total_timesteps=training_steps)\n",
        "#evaluating performance\n",
        "gaereward_mean, gaereward_std= evaluate_policy(gae_model, evaluation_env, n_eval_episodes=100)\n"
      ],
      "metadata": {
        "id": "buUHwpiePgIR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. n-STEP MODEL"
      ],
      "metadata": {
        "id": "GQdre26qQEh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training \n",
        "nstep_model.learn(total_timesteps=training_steps)\n",
        "#evaluating performance\n",
        "nstepreward_mean, nstepreward_std= evaluate_policy(nstep_model, evaluation_env, n_eval_episodes=100)\n"
      ],
      "metadata": {
        "id": "Dtk25cdMQRdk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Results"
      ],
      "metadata": {
        "id": "nxP_vg0VQpTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Mean Reward of Vanilla model: {np.round(vanillareward_mean,2)} +/- {np.round(vanillareward_std,2)}')\n",
        "print(f'Mean Reward of N-Step model: {np.round(nstepreward_mean,2)} +/- {np.round(nstepreward_std,2)}')\n",
        "print(f'Mean Reward of GAE model: {np.round(gaereward_mean,2)} +/- {np.round(gaereward_std,2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WOb4wv9QrX0",
        "outputId": "ba94a9fc-1fee-4acc-9d4a-fe6f6e3aff48"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Reward of Vanilla model: 52.31 +/- 22.15\n",
            "Mean Reward of N-Step model: 379.06 +/- 63.69\n",
            "Mean Reward of GAE model: 500.0 +/- 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analysis\n",
        "From the analysis we can see that the GAE model has the best performance with the mean reward of 500. The N-Step is the second best model and fits with a mean reward of 379.06 Also, the Vanilla model performs the worst with a mean reward of 52.31.  These values will likely vary as the cells are rerun. \n"
      ],
      "metadata": {
        "id": "LUrEBhpgaafJ"
      }
    }
  ]
}